{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "pytorch_notebooks",
   "display_name": "pytorch_notebooks"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory + Math\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning** is about finding **w** that minimises the loss\n",
    "> arg min loss(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent intuition:\n",
    "\n",
    "<img src=\"images/gradient_descent.gif\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically,\n",
    "\n",
    "$$loss = (\\hat y - y)^2 = (x \\ast w - y)^2$$\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\ast \\frac{\\partial loss}{\\partial w} $$\n",
    "\n",
    "Using calculus (or https://www.derivative-calculator.net) we can show that:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial w} = \\frac{\\partial}{\\partial w} [(x \\ast w - y)^2] = 2w\\left(wx-y\\right)\n",
    "$$\n",
    "\n",
    "Therefore, the learning equation is as follows:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\ast 2w\\left(wx-y\\right) $$\n",
    "\n",
    "where $\\alpha$ is the **learning rate**, a **hyperparameter** of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the step-by-step walkthrough of the derivation:\n",
    "\n",
    "<img src=\"images/loss_function_derivative.jpg\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will re-use the setup of our [01_supervised_learning.ipynb](01_supervised_learning.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randonmly initialise weight w\n",
    "w = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define forward-pass funtion\n",
    "def forward(x):\n",
    "    return round((x * w), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def loss(x, y):\n",
    "    y_hat = forward(x)\n",
    "    return round(((y_hat - y) * (y_hat - y)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our `gradient()` function and re-write the training loop to make use of the `gradient()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient\n",
    "def gradient(x, y):\n",
    "    return 2 * x * (x * w - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "predict (before training) 4 4.0\n\t grad:  1.0 2.0 -2.0 0.96\n\t grad:  2.0 4.0 -7.84 3.24\n\t grad:  3.0 6.0 -16.2288 4.93\nprogress:  0 w= 1.260688 loss= 4.93\n\t grad:  1.0 2.0 -1.478624 0.52\n\t grad:  2.0 4.0 -5.796206079999999 1.77\n\t grad:  3.0 6.0 -11.998146585599997 2.69\nprogress:  1 w= 1.453417766656 loss= 2.69\n\t grad:  1.0 2.0 -1.093164466688 0.29\n\t grad:  2.0 4.0 -4.285204709416961 0.98\n\t grad:  3.0 6.0 -8.87037374849311 1.46\nprogress:  2 w= 1.5959051959019805 loss= 1.46\n\t grad:  1.0 2.0 -0.8081896081960389 0.16\n\t grad:  2.0 4.0 -3.1681032641284723 0.53\n\t grad:  3.0 6.0 -6.557973756745939 0.81\nprogress:  3 w= 1.701247862192685 loss= 0.81\n\t grad:  1.0 2.0 -0.59750427561463 0.08\n\t grad:  2.0 4.0 -2.3422167604093502 0.29\n\t grad:  3.0 6.0 -4.848388694047353 0.44\nprogress:  4 w= 1.7791289594933983 loss= 0.44\n\t grad:  1.0 2.0 -0.44174208101320334 0.05\n\t grad:  2.0 4.0 -1.7316289575717576 0.16\n\t grad:  3.0 6.0 -3.584471942173538 0.24\nprogress:  5 w= 1.836707389300983 loss= 0.24\n\t grad:  1.0 2.0 -0.3265852213980338 0.03\n\t grad:  2.0 4.0 -1.2802140678802925 0.08\n\t grad:  3.0 6.0 -2.650043120512205 0.13\nprogress:  6 w= 1.8792758133988885 loss= 0.13\n\t grad:  1.0 2.0 -0.241448373202223 0.01\n\t grad:  2.0 4.0 -0.946477622952715 0.05\n\t grad:  3.0 6.0 -1.9592086795121197 0.07\nprogress:  7 w= 1.910747160155559 loss= 0.07\n\t grad:  1.0 2.0 -0.17850567968888198 0.01\n\t grad:  2.0 4.0 -0.6997422643804168 0.03\n\t grad:  3.0 6.0 -1.4484664872674653 0.04\nprogress:  8 w= 1.9340143044689266 loss= 0.04\n\t grad:  1.0 2.0 -0.13197139106214673 0.0\n\t grad:  2.0 4.0 -0.5173278529636143 0.01\n\t grad:  3.0 6.0 -1.0708686556346834 0.02\nprogress:  9 w= 1.9512159834655312 loss= 0.02\n\t grad:  1.0 2.0 -0.09756803306893769 0.0\n\t grad:  2.0 4.0 -0.38246668963023644 0.01\n\t grad:  3.0 6.0 -0.7917060475345892 0.01\nprogress:  10 w= 1.9639333911678687 loss= 0.01\n\t grad:  1.0 2.0 -0.07213321766426262 0.0\n\t grad:  2.0 4.0 -0.2827622132439096 0.0\n\t grad:  3.0 6.0 -0.5853177814148953 0.01\nprogress:  11 w= 1.9733355232910992 loss= 0.01\n\t grad:  1.0 2.0 -0.05332895341780164 0.0\n\t grad:  2.0 4.0 -0.2090494973977819 0.0\n\t grad:  3.0 6.0 -0.4327324596134101 0.0\nprogress:  12 w= 1.9802866323953892 loss= 0.0\n\t grad:  1.0 2.0 -0.039426735209221686 0.0\n\t grad:  2.0 4.0 -0.15455280202014876 0.0\n\t grad:  3.0 6.0 -0.3199243001817109 0.0\nprogress:  13 w= 1.9854256707695 loss= 0.0\n\t grad:  1.0 2.0 -0.02914865846100012 0.0\n\t grad:  2.0 4.0 -0.11426274116712065 0.0\n\t grad:  3.0 6.0 -0.2365238742159388 0.0\nprogress:  14 w= 1.9892250235079405 loss= 0.0\n\t grad:  1.0 2.0 -0.021549952984118992 0.0\n\t grad:  2.0 4.0 -0.08447581569774698 0.0\n\t grad:  3.0 6.0 -0.17486493849433593 0.0\nprogress:  15 w= 1.9920339305797026 loss= 0.0\n\t grad:  1.0 2.0 -0.015932138840594856 0.0\n\t grad:  2.0 4.0 -0.062453984255132156 0.0\n\t grad:  3.0 6.0 -0.12927974740812687 0.0\nprogress:  16 w= 1.994110589284741 loss= 0.0\n\t grad:  1.0 2.0 -0.011778821430517894 0.0\n\t grad:  2.0 4.0 -0.046172980007630926 0.0\n\t grad:  3.0 6.0 -0.09557806861579543 0.0\nprogress:  17 w= 1.9956458879852805 loss= 0.0\n\t grad:  1.0 2.0 -0.008708224029438938 0.0\n\t grad:  2.0 4.0 -0.03413623819540135 0.0\n\t grad:  3.0 6.0 -0.07066201306448505 0.0\nprogress:  18 w= 1.9967809527381737 loss= 0.0\n\t grad:  1.0 2.0 -0.006438094523652627 0.0\n\t grad:  2.0 4.0 -0.02523733053271826 0.0\n\t grad:  3.0 6.0 -0.052241274202728505 0.0\nprogress:  19 w= 1.9976201197307648 loss= 0.0\n\t grad:  1.0 2.0 -0.004759760538470381 0.0\n\t grad:  2.0 4.0 -0.01865826131080439 0.0\n\t grad:  3.0 6.0 -0.03862260091336722 0.0\nprogress:  20 w= 1.998240525958391 loss= 0.0\n\t grad:  1.0 2.0 -0.0035189480832178432 0.0\n\t grad:  2.0 4.0 -0.01379427648621423 0.0\n\t grad:  3.0 6.0 -0.028554152326460525 0.0\nprogress:  21 w= 1.99869919972735 loss= 0.0\n\t grad:  1.0 2.0 -0.002601600545300009 0.0\n\t grad:  2.0 4.0 -0.01019827413757568 0.0\n\t grad:  3.0 6.0 -0.021110427464781978 0.0\nprogress:  22 w= 1.9990383027488265 loss= 0.0\n\t grad:  1.0 2.0 -0.001923394502346909 0.0\n\t grad:  2.0 4.0 -0.007539706449199102 0.0\n\t grad:  3.0 6.0 -0.01560719234984198 0.0\nprogress:  23 w= 1.9992890056818404 loss= 0.0\n\t grad:  1.0 2.0 -0.0014219886363191492 0.0\n\t grad:  2.0 4.0 -0.005574195454370212 0.0\n\t grad:  3.0 6.0 -0.011538584590544687 0.0\nprogress:  24 w= 1.999474353368653 loss= 0.0\n\t grad:  1.0 2.0 -0.0010512932626940419 0.0\n\t grad:  2.0 4.0 -0.004121069589761106 0.0\n\t grad:  3.0 6.0 -0.008530614050808794 0.0\nprogress:  25 w= 1.9996113831376856 loss= 0.0\n\t grad:  1.0 2.0 -0.0007772337246287897 0.0\n\t grad:  2.0 4.0 -0.0030467562005451754 0.0\n\t grad:  3.0 6.0 -0.006306785335127074 0.0\nprogress:  26 w= 1.9997126908902887 loss= 0.0\n\t grad:  1.0 2.0 -0.0005746182194226179 0.0\n\t grad:  2.0 4.0 -0.002252503420136165 0.0\n\t grad:  3.0 6.0 -0.00466268207967957 0.0\nprogress:  27 w= 1.9997875889274812 loss= 0.0\n\t grad:  1.0 2.0 -0.0004248221450375844 0.0\n\t grad:  2.0 4.0 -0.0016653028085471533 0.0\n\t grad:  3.0 6.0 -0.0034471768136938863 0.0\nprogress:  28 w= 1.9998429619451539 loss= 0.0\n\t grad:  1.0 2.0 -0.00031407610969225175 0.0\n\t grad:  2.0 4.0 -0.0012311783499932005 0.0\n\t grad:  3.0 6.0 -0.0025485391844828342 0.0\nprogress:  29 w= 1.9998838998815958 loss= 0.0\n\t grad:  1.0 2.0 -0.00023220023680847746 0.0\n\t grad:  2.0 4.0 -0.0009102249282886277 0.0\n\t grad:  3.0 6.0 -0.0018841656015560204 0.0\nprogress:  30 w= 1.9999141657892625 loss= 0.0\n\t grad:  1.0 2.0 -0.00017166842147497974 0.0\n\t grad:  2.0 4.0 -0.0006729402121816719 0.0\n\t grad:  3.0 6.0 -0.0013929862392156878 0.0\nprogress:  31 w= 1.9999365417379913 loss= 0.0\n\t grad:  1.0 2.0 -0.0001269165240174175 0.0\n\t grad:  2.0 4.0 -0.0004975127741477792 0.0\n\t grad:  3.0 6.0 -0.0010298514424817995 0.0\nprogress:  32 w= 1.9999530845453979 loss= 0.0\n\t grad:  1.0 2.0 -9.383090920422887e-05 0.0\n\t grad:  2.0 4.0 -0.00036781716408107457 0.0\n\t grad:  3.0 6.0 -0.0007613815296476645 0.0\nprogress:  33 w= 1.9999653148414271 loss= 0.0\n\t grad:  1.0 2.0 -6.937031714571162e-05 0.0\n\t grad:  2.0 4.0 -0.0002719316432120422 0.0\n\t grad:  3.0 6.0 -0.0005628985014531906 0.0\nprogress:  34 w= 1.999974356846045 loss= 0.0\n\t grad:  1.0 2.0 -5.1286307909848006e-05 0.0\n\t grad:  2.0 4.0 -0.00020104232700646207 0.0\n\t grad:  3.0 6.0 -0.0004161576169003922 0.0\nprogress:  35 w= 1.9999810417085633 loss= 0.0\n\t grad:  1.0 2.0 -3.7916582873442906e-05 0.0\n\t grad:  2.0 4.0 -0.0001486330048638962 0.0\n\t grad:  3.0 6.0 -0.0003076703200690645 0.0\nprogress:  36 w= 1.9999859839076413 loss= 0.0\n\t grad:  1.0 2.0 -2.8032184717474706e-05 0.0\n\t grad:  2.0 4.0 -0.0001098861640933535 0.0\n\t grad:  3.0 6.0 -0.00022746435967313516 0.0\nprogress:  37 w= 1.9999896377347262 loss= 0.0\n\t grad:  1.0 2.0 -2.0724530547688857e-05 0.0\n\t grad:  2.0 4.0 -8.124015974608767e-05 0.0\n\t grad:  3.0 6.0 -0.00016816713067413502 0.0\nprogress:  38 w= 1.999992339052936 loss= 0.0\n\t grad:  1.0 2.0 -1.5321894128117464e-05 0.0\n\t grad:  2.0 4.0 -6.006182498197177e-05 0.0\n\t grad:  3.0 6.0 -0.00012432797771566584 0.0\nprogress:  39 w= 1.9999943361699042 loss= 0.0\n\t grad:  1.0 2.0 -1.1327660191629008e-05 0.0\n\t grad:  2.0 4.0 -4.4404427951505454e-05 0.0\n\t grad:  3.0 6.0 -9.191716585732479e-05 0.0\nprogress:  40 w= 1.9999958126624442 loss= 0.0\n\t grad:  1.0 2.0 -8.37467511161094e-06 0.0\n\t grad:  2.0 4.0 -3.282872643772805e-05 0.0\n\t grad:  3.0 6.0 -6.795546372551087e-05 0.0\nprogress:  41 w= 1.999996904251097 loss= 0.0\n\t grad:  1.0 2.0 -6.191497806007362e-06 0.0\n\t grad:  2.0 4.0 -2.4270671399762023e-05 0.0\n\t grad:  3.0 6.0 -5.0240289795056015e-05 0.0\nprogress:  42 w= 1.999997711275687 loss= 0.0\n\t grad:  1.0 2.0 -4.5774486259198e-06 0.0\n\t grad:  2.0 4.0 -1.794359861406747e-05 0.0\n\t grad:  3.0 6.0 -3.714324913239864e-05 0.0\nprogress:  43 w= 1.9999983079186507 loss= 0.0\n\t grad:  1.0 2.0 -3.3841626985164908e-06 0.0\n\t grad:  2.0 4.0 -1.326591777761621e-05 0.0\n\t grad:  3.0 6.0 -2.7460449796734565e-05 0.0\nprogress:  44 w= 1.9999987490239537 loss= 0.0\n\t grad:  1.0 2.0 -2.5019520926150562e-06 0.0\n\t grad:  2.0 4.0 -9.807652203264183e-06 0.0\n\t grad:  3.0 6.0 -2.0301840059744336e-05 0.0\nprogress:  45 w= 1.9999990751383971 loss= 0.0\n\t grad:  1.0 2.0 -1.8497232057157476e-06 0.0\n\t grad:  2.0 4.0 -7.250914967116273e-06 0.0\n\t grad:  3.0 6.0 -1.5009393983689279e-05 0.0\nprogress:  46 w= 1.9999993162387186 loss= 0.0\n\t grad:  1.0 2.0 -1.3675225627451937e-06 0.0\n\t grad:  2.0 4.0 -5.3606884460322135e-06 0.0\n\t grad:  3.0 6.0 -1.109662508014253e-05 0.0\nprogress:  47 w= 1.9999994944870796 loss= 0.0\n\t grad:  1.0 2.0 -1.0110258408246864e-06 0.0\n\t grad:  2.0 4.0 -3.963221296032771e-06 0.0\n\t grad:  3.0 6.0 -8.20386808086937e-06 0.0\nprogress:  48 w= 1.9999996262682318 loss= 0.0\n\t grad:  1.0 2.0 -7.474635363990956e-07 0.0\n\t grad:  2.0 4.0 -2.930057062755509e-06 0.0\n\t grad:  3.0 6.0 -6.065218119744031e-06 0.0\nprogress:  49 w= 1.999999723695619 loss= 0.0\n\t grad:  1.0 2.0 -5.526087618612507e-07 0.0\n\t grad:  2.0 4.0 -2.166226346744793e-06 0.0\n\t grad:  3.0 6.0 -4.484088535150477e-06 0.0\nprogress:  50 w= 1.9999997957248556 loss= 0.0\n\t grad:  1.0 2.0 -4.08550288710785e-07 0.0\n\t grad:  2.0 4.0 -1.6015171322436572e-06 0.0\n\t grad:  3.0 6.0 -3.3151404608133817e-06 0.0\nprogress:  51 w= 1.9999998489769344 loss= 0.0\n\t grad:  1.0 2.0 -3.020461312175371e-07 0.0\n\t grad:  2.0 4.0 -1.1840208351543424e-06 0.0\n\t grad:  3.0 6.0 -2.4509231284497446e-06 0.0\nprogress:  52 w= 1.9999998883468353 loss= 0.0\n\t grad:  1.0 2.0 -2.2330632942768602e-07 0.0\n\t grad:  2.0 4.0 -8.753608113920563e-07 0.0\n\t grad:  3.0 6.0 -1.811996877876254e-06 0.0\nprogress:  53 w= 1.9999999174534755 loss= 0.0\n\t grad:  1.0 2.0 -1.6509304900935717e-07 0.0\n\t grad:  2.0 4.0 -6.471647520100987e-07 0.0\n\t grad:  3.0 6.0 -1.3396310407642886e-06 0.0\nprogress:  54 w= 1.999999938972364 loss= 0.0\n\t grad:  1.0 2.0 -1.220552721115098e-07 0.0\n\t grad:  2.0 4.0 -4.784566662863199e-07 0.0\n\t grad:  3.0 6.0 -9.904052991061008e-07 0.0\nprogress:  55 w= 1.9999999548815364 loss= 0.0\n\t grad:  1.0 2.0 -9.023692726373156e-08 0.0\n\t grad:  2.0 4.0 -3.5372875473171916e-07 0.0\n\t grad:  3.0 6.0 -7.322185204827747e-07 0.0\nprogress:  56 w= 1.9999999666433785 loss= 0.0\n\t grad:  1.0 2.0 -6.671324292994996e-08 0.0\n\t grad:  2.0 4.0 -2.615159129248923e-07 0.0\n\t grad:  3.0 6.0 -5.413379398078177e-07 0.0\nprogress:  57 w= 1.9999999753390494 loss= 0.0\n\t grad:  1.0 2.0 -4.932190122985958e-08 0.0\n\t grad:  2.0 4.0 -1.9334185274999527e-07 0.0\n\t grad:  3.0 6.0 -4.002176350326181e-07 0.0\nprogress:  58 w= 1.9999999817678633 loss= 0.0\n\t grad:  1.0 2.0 -3.6464273378555845e-08 0.0\n\t grad:  2.0 4.0 -1.429399514307761e-07 0.0\n\t grad:  3.0 6.0 -2.9588569994132286e-07 0.0\nprogress:  59 w= 1.9999999865207625 loss= 0.0\n\t grad:  1.0 2.0 -2.6958475007887728e-08 0.0\n\t grad:  2.0 4.0 -1.0567722164012139e-07 0.0\n\t grad:  3.0 6.0 -2.1875184863517916e-07 0.0\nprogress:  60 w= 1.999999990034638 loss= 0.0\n\t grad:  1.0 2.0 -1.993072418216002e-08 0.0\n\t grad:  2.0 4.0 -7.812843882959442e-08 0.0\n\t grad:  3.0 6.0 -1.617258700292723e-07 0.0\nprogress:  61 w= 1.9999999926324883 loss= 0.0\n\t grad:  1.0 2.0 -1.473502342363986e-08 0.0\n\t grad:  2.0 4.0 -5.7761292637792394e-08 0.0\n\t grad:  3.0 6.0 -1.195658771990793e-07 0.0\nprogress:  62 w= 1.99999999455311 loss= 0.0\n\t grad:  1.0 2.0 -1.0893780100218464e-08 0.0\n\t grad:  2.0 4.0 -4.270361841918202e-08 0.0\n\t grad:  3.0 6.0 -8.839649012770678e-08 0.0\nprogress:  63 w= 1.9999999959730488 loss= 0.0\n\t grad:  1.0 2.0 -8.05390243385773e-09 0.0\n\t grad:  2.0 4.0 -3.1571296688071016e-08 0.0\n\t grad:  3.0 6.0 -6.53525820126788e-08 0.0\nprogress:  64 w= 1.9999999970228268 loss= 0.0\n\t grad:  1.0 2.0 -5.9543463493128e-09 0.0\n\t grad:  2.0 4.0 -2.334103754719763e-08 0.0\n\t grad:  3.0 6.0 -4.8315948575350376e-08 0.0\nprogress:  65 w= 1.9999999977989402 loss= 0.0\n\t grad:  1.0 2.0 -4.402119557767037e-09 0.0\n\t grad:  2.0 4.0 -1.725630838222969e-08 0.0\n\t grad:  3.0 6.0 -3.5720557178819945e-08 0.0\nprogress:  66 w= 1.9999999983727301 loss= 0.0\n\t grad:  1.0 2.0 -3.254539748809293e-09 0.0\n\t grad:  2.0 4.0 -1.2757796596929438e-08 0.0\n\t grad:  3.0 6.0 -2.6408640607655798e-08 0.0\nprogress:  67 w= 1.9999999987969397 loss= 0.0\n\t grad:  1.0 2.0 -2.406120636067044e-09 0.0\n\t grad:  2.0 4.0 -9.431992964437086e-09 0.0\n\t grad:  3.0 6.0 -1.9524227568012975e-08 0.0\nprogress:  68 w= 1.999999999110563 loss= 0.0\n\t grad:  1.0 2.0 -1.7788739370416806e-09 0.0\n\t grad:  2.0 4.0 -6.97318647269185e-09 0.0\n\t grad:  3.0 6.0 -1.4434496264925656e-08 0.0\nprogress:  69 w= 1.9999999993424284 loss= 0.0\n\t grad:  1.0 2.0 -1.3151431055291596e-09 0.0\n\t grad:  2.0 4.0 -5.155360582875801e-09 0.0\n\t grad:  3.0 6.0 -1.067159693945996e-08 0.0\nprogress:  70 w= 1.9999999995138495 loss= 0.0\n\t grad:  1.0 2.0 -9.72300906454393e-10 0.0\n\t grad:  2.0 4.0 -3.811418736177075e-09 0.0\n\t grad:  3.0 6.0 -7.88963561149103e-09 0.0\nprogress:  71 w= 1.9999999996405833 loss= 0.0\n\t grad:  1.0 2.0 -7.18833437218791e-10 0.0\n\t grad:  2.0 4.0 -2.8178277489132597e-09 0.0\n\t grad:  3.0 6.0 -5.832902161273523e-09 0.0\nprogress:  72 w= 1.999999999734279 loss= 0.0\n\t grad:  1.0 2.0 -5.314420015167798e-10 0.0\ngrad:  2.0 4.0 -2.0832526814729135e-09 0.0\n\t grad:  3.0 6.0 -4.31233715403323e-09 0.0\nprogress:  73 w= 1.9999999998035491 loss= 0.0\n\t grad:  1.0 2.0 -3.92901711165905e-10 0.0\n\t grad:  2.0 4.0 -1.5401742103904326e-09 0.0\n\t grad:  3.0 6.0 -3.188159070077745e-09 0.0\nprogress:  74 w= 1.9999999998547615 loss= 0.0\n\t grad:  1.0 2.0 -2.9047697580608656e-10 0.0\n\t grad:  2.0 4.0 -1.1386696030513122e-09 0.0\n\t grad:  3.0 6.0 -2.3570478902001923e-09 0.0\nprogress:  75 w= 1.9999999998926234 loss= 0.0\n\t grad:  1.0 2.0 -2.1475310418850313e-10 0.0\n\t grad:  2.0 4.0 -8.418314934033333e-10 0.0\n\t grad:  3.0 6.0 -1.7425900722400911e-09 0.0\nprogress:  76 w= 1.9999999999206153 loss= 0.0\n\t grad:  1.0 2.0 -1.5876944203796484e-10 0.0\n\t grad:  2.0 4.0 -6.223768167501476e-10 0.0\n\t grad:  3.0 6.0 -1.2883241140571045e-09 0.0\nprogress:  77 w= 1.9999999999413098 loss= 0.0\n\t grad:  1.0 2.0 -1.17380327679939e-10 0.0\n\t grad:  2.0 4.0 -4.601314884666863e-10 0.0\n\t grad:  3.0 6.0 -9.524754318590567e-10 0.0\nprogress:  78 w= 1.9999999999566096 loss= 0.0\n\t grad:  1.0 2.0 -8.678080476443029e-11 0.0\n\t grad:  2.0 4.0 -3.4018121652934497e-10 0.0\n\t grad:  3.0 6.0 -7.041780492045291e-10 0.0\nprogress:  79 w= 1.9999999999679208 loss= 0.0\n\t grad:  1.0 2.0 -6.415845632545825e-11 0.0\n\t grad:  2.0 4.0 -2.5150193039280566e-10 0.0\n\t grad:  3.0 6.0 -5.206075570640678e-10 0.0\nprogress:  80 w= 1.9999999999762834 loss= 0.0\n\t grad:  1.0 2.0 -4.743316850408519e-11 0.0\n\t grad:  2.0 4.0 -1.8593837580738182e-10 0.0\n\t grad:  3.0 6.0 -3.8489211817704927e-10 0.0\nprogress:  81 w= 1.999999999982466 loss= 0.0\n\t grad:  1.0 2.0 -3.5067948545020045e-11 0.0\n\t grad:  2.0 4.0 -1.3746692673066718e-10 0.0\n\t grad:  3.0 6.0 -2.845563784603655e-10 0.0\nprogress:  82 w= 1.9999999999870368 loss= 0.0\n\t grad:  1.0 2.0 -2.5926372160256506e-11 0.0\n\t grad:  2.0 4.0 -1.0163070385260653e-10 0.0\n\t grad:  3.0 6.0 -2.1037571684701106e-10 0.0\nprogress:  83 w= 1.999999999990416 loss= 0.0\n\t grad:  1.0 2.0 -1.9167778475548403e-11 0.0\n\t grad:  2.0 4.0 -7.51381179497912e-11 0.0\n\t grad:  3.0 6.0 -1.5553425214420713e-10 0.0\nprogress:  84 w= 1.9999999999929146 loss= 0.0\n\t grad:  1.0 2.0 -1.4170886686315498e-11 0.0\n\t grad:  2.0 4.0 -5.555023108172463e-11 0.0\n\t grad:  3.0 6.0 -1.1499068364173581e-10 0.0\nprogress:  85 w= 1.9999999999947617 loss= 0.0\n\t grad:  1.0 2.0 -1.0476508549572827e-11 0.0\n\t grad:  2.0 4.0 -4.106759377009439e-11 0.0\n\t grad:  3.0 6.0 -8.500933290633839e-11 0.0\nprogress:  86 w= 1.9999999999961273 loss= 0.0\n\t grad:  1.0 2.0 -7.745359908994942e-12 0.0\n\t grad:  2.0 4.0 -3.036149109902908e-11 0.0\n\t grad:  3.0 6.0 -6.285105769165966e-11 0.0\nprogress:  87 w= 1.999999999997137 loss= 0.0\n\t grad:  1.0 2.0 -5.726086271806707e-12 0.0\n\t grad:  2.0 4.0 -2.2446045022661565e-11 0.0\n\t grad:  3.0 6.0 -4.646416584819235e-11 0.0\nprogress:  88 w= 1.9999999999978835 loss= 0.0\n\t grad:  1.0 2.0 -4.233058348290797e-12 0.0\n\t grad:  2.0 4.0 -1.659294923683774e-11 0.0\n\t grad:  3.0 6.0 -3.4351188560322043e-11 0.0\nprogress:  89 w= 1.9999999999984353 loss= 0.0\n\t grad:  1.0 2.0 -3.1294966618133913e-12 0.0\n\t grad:  2.0 4.0 -1.226752033289813e-11 0.0\n\t grad:  3.0 6.0 -2.539835008974478e-11 0.0\nprogress:  90 w= 1.9999999999988431 loss= 0.0\n\t grad:  1.0 2.0 -2.3137047833188262e-12 0.0\n\t grad:  2.0 4.0 -9.070078021977679e-12 0.0\n\t grad:  3.0 6.0 -1.8779644506139448e-11 0.0\nprogress:  91 w= 1.9999999999991447 loss= 0.0\n\t grad:  1.0 2.0 -1.7106316363424412e-12 0.0\n\t grad:  2.0 4.0 -6.7057470687359455e-12 0.0\n\t grad:  3.0 6.0 -1.3882228699912957e-11 0.0\nprogress:  92 w= 1.9999999999993676 loss= 0.0\n\t grad:  1.0 2.0 -1.2647660696529783e-12 0.0\n\t grad:  2.0 4.0 -4.957811938766099e-12 0.0\n\t grad:  3.0 6.0 -1.0263789818054647e-11 0.0\nprogress:  93 w= 1.9999999999995324 loss= 0.0\n\t grad:  1.0 2.0 -9.352518759442319e-13 0.0\n\t grad:  2.0 4.0 -3.666400516522117e-12 0.0\n\t grad:  3.0 6.0 -7.58859641791787e-12 0.0\nprogress:  94 w= 1.9999999999996543 loss= 0.0\n\t grad:  1.0 2.0 -6.914468997365475e-13 0.0\n\t grad:  2.0 4.0 -2.7107205369247822e-12 0.0\n\t grad:  3.0 6.0 -5.611511255665391e-12 0.0\nprogress:  95 w= 1.9999999999997444 loss= 0.0\n\t grad:  1.0 2.0 -5.111466805374221e-13 0.0\n\t grad:  2.0 4.0 -2.0037305148434825e-12 0.0\n\t grad:  3.0 6.0 -4.1460168631601846e-12 0.0\nprogress:  96 w= 1.999999999999811 loss= 0.0\n\t grad:  1.0 2.0 -3.779199175824033e-13 0.0\n\t grad:  2.0 4.0 -1.4814816040598089e-12 0.0\n\t grad:  3.0 6.0 -3.064215547965432e-12 0.0\nprogress:  97 w= 1.9999999999998603 loss= 0.0\n\t grad:  1.0 2.0 -2.793321129956894e-13 0.0\n\t grad:  2.0 4.0 -1.0942358130705543e-12 0.0\n\t grad:  3.0 6.0 -2.2648549702353193e-12 0.0\nprogress:  98 w= 1.9999999999998967 loss= 0.0\n\t grad:  1.0 2.0 -2.0650148258027912e-13 0.0\n\t grad:  2.0 4.0 -8.100187187665142e-13 0.0\n\t grad:  3.0 6.0 -1.6786572132332367e-12 0.0\nprogress:  99 w= 1.9999999999999236 loss= 0.0\npredict (after training) 4 8.0\n"
    }
   ],
   "source": [
    "# learning using gradient descent\n",
    "\n",
    "## before training \n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "## learning loop\n",
    "alpha = 0.01\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - alpha * grad\n",
    "        l = loss(x_val, y_val)\n",
    "        print(\"\\t\", \"grad: \", x_val, y_val, grad, l)\n",
    "    print(\"progress: \", epoch, \"w=\", w, \"loss=\", l)\n",
    "\n",
    "## after training\n",
    "print(\"predict (after training)\", 4, forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement for non-linear mapping: \n",
    "$$ \\hat{y} = x^2 \\ast w_2 + x \\ast w_1 + b $$\n",
    "$$ loss = (\\hat{y} - y)^2 $$\n",
    "$$ \\frac {\\partial}{\\partial w_1} loss = x $$\n",
    "$$ \\frac {\\partial}{\\partial w_2} loss = x^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randonmly initialise weight w\n",
    "w_1 = 1.0\n",
    "w_2 = 1.0\n",
    "# b = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define forward-pass funtion\n",
    "def forward(x):\n",
    "    return ((x * x * w_2) + (x * w_1))\n",
    "    # return ((x * x * w_2) + (x * w_1) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def loss(x, y):\n",
    "    y_hat = forward(x)\n",
    "    return round(((y_hat - y) * (y_hat - y)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient\n",
    "def gradient_w1(x, y):\n",
    "    return x\n",
    "\n",
    "def gradient_w2(x, y):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "predict (before training) 4 20.0\n\t grad:  1.0 2.0 1.0 1.0\n\t grad:  2.0 4.0 2.0 4.0\n\t grad:  3.0 6.0 3.0 9.0\nprogress:  0 w1= 0.94 w2= 0.86 loss= 20.79\n\t grad:  1.0 2.0 1.0 1.0\n\t grad:  2.0 4.0 2.0 4.0\n\t grad:  3.0 6.0 3.0 9.0\nprogress:  1 w1= 0.8799999999999999 w2= 0.72 loss= 9.73\n\t grad:  1.0 2.0 1.0 1.0\n\t grad:  2.0 4.0 2.0 4.0\n\t grad:  3.0 6.0 3.0 9.0\nprogress:  2 w1= 0.8199999999999998 w2= 0.58 loss= 2.82\n\t grad:  1.0 2.0 1.0 1.0\n\t grad:  2.0 4.0 2.0 4.0\n\t grad:  3.0 6.0 3.0 9.0\nprogress:  3 w1= 0.7599999999999998 w2= 0.43999999999999995 loss= 0.06\npredict (after training) 4 10.079999999999998\n"
    }
   ],
   "source": [
    "# learning using gradient descent\n",
    "threshold = 0.1\n",
    "\n",
    "## before training \n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "## learning loop\n",
    "alpha = 0.01\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad_w1 = gradient_w1(x_val, y_val)\n",
    "        grad_w2 = gradient_w2(x_val, y_val)\n",
    "        w_1 = w_1 - alpha * grad_w1\n",
    "        w_2 = w_2 - alpha * grad_w2\n",
    "        l = loss(x_val, y_val)\n",
    "        print(\"\\t\", \"grad: \", x_val, y_val, grad_w1, grad_w2)\n",
    "    print(\"progress: \", epoch, \"w1=\", w_1, \"w2=\", w_2, \"loss=\", l)\n",
    "    if l < threshold:\n",
    "        break\n",
    "\n",
    "## after training\n",
    "print(\"predict (after training)\", 4, forward(4))"
   ]
  }
 ]
}